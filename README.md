# Proyecto Machine Learning Techniques: Descriptor de alrededores para individuos con discapacidad visual

La discapacidad visual se refiere a la pérdida de visión o la incapacidad de ver el entorno circundante \cite{ML_sight}. Según la Organización Mundial de la Salud (OMS), en el mundo hay al menos 2200 millones de personas con deterioro de la visión cercana o distante \cite{who_2021, blindness}. De hecho, según el Censo DANE 2018, en Colombia viven 1.948.332 personas ciegas o con baja visión \cite{ceguera_colombia}. 

El deterioro de la visión afecta gravemente a la calidad de vida. Las tasas de participación en el mercado laboral y de productividad de los adultos con deterioro de la visión a menudo son más bajas y suelen registrar tasas más altas de depresión y ansiedad. En el caso de los adultos mayores, el deterioro de la visión puede contribuir al aislamiento social, a la dificultad para caminar, a un mayor riesgo de caídas y fracturas, y a una mayor probabilidad de ingreso temprano en residencias de ancianos. 

Existen herramientas tradicionales, como el braille y el bastón blanco, que facilitan el desarrollo de la población con discapacidad visual. No obstante, no existen tecnologías ni herramientas que le permitan a las personas con discapacidad visual entender su entorno a medianas y grandes distancias, y así orientarse y mejorar su capacidad de navegación.

Unido a esto, la población ciega o con baja visión vive en un mundo diseñado por personas con buena visión, lo cual les genera dificultades o desventajas en su día a día. Si bien existen herramientas que buscan describir lo que se puede observar en una imagen o video - en particular en redes sociales -, muchas herramientas excluyen el estudio y la definición del entorno para esta población, sobre todo en ambientes exteriores (como en las calles), que son circunstancias a las que día a día se ve expuesta esta población.

Esta investigación pretende diseñar e implementar una herramienta capaz de describir lo que ocurre en una imagen tomada por un usuario ciego o con dificultades de visión, con el fin de brindar información sobre lo que está ocurriendo en sus alrededores, que puede serle de utilidad para decidir su siguiente paso. En específico, se diseña y construye un modelo de aprendizaje automático que genere una descripción escrita a partir de una imagen y, posteriormente, conecte el modelo con la API \textit{text to speech} de Google\footnote{\url{https://cloud.google.com/text-to-speech}} con el fin de brindar información auditiva al usuario sobre lo que ocurre a su alrededor.
